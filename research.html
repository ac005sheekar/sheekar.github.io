<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Research - Sheekar Banerjee</title>
  
  <meta name="author" content="Sheekar Banerjee">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css">
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.7.1/dist/jquery.slim.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
  <link rel="icon" type="image/png" href="sheekar2.jpg">
</head>

<body>
  <nav class="navbar navbar-expand-sm bg-dark navbar-dark">
    <!-- Brand/logo -->
    <a class="navbar-brand" href="index.html">Home</a>
    
    <!-- Links -->
    <ul class="navbar-nav">
      <li class="nav-item">
        <a class="nav-link" href="research.html">Research</a>
      </li>
      <li class="nav-item">
        <a class="nav-link" href="projects.html">Projects</a>
      </li>
      <li class="nav-item">
        <a class="nav-link" href="teach.html">Teaching</a>
      </li>
      <li class="nav-item">
        <a class="nav-link" href="leader.html">Leadership</a>
      </li>
      <li class="nav-item">
        <a class="nav-link" href="workshops.html">Workshops</a>
      </li>
      <li class="nav-item">
        <a class="nav-link" href="culture.html">Culture</a>
      </li>
    </ul>
  </nav><br>
  
        <table style="width:75%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">

              <heading><strong>Research Interest</strong></heading><br><br>
              <a href="interest.PNG"><img style="width:30%;max-width:30%" alt="profile photo" src="interest.PNG" class="hoverZoomLink"></a>
            </td>
              <p style="text-align: center;">
                
                <!-- My research interests lie broadly in computer vision and artificial intelligence. My current focus majorly is to explore and conduct fundamental computer vision research with limited supervision, with a goal to conduct research and design products benefiting humanity. I am excited to be part of this fast-evolving and fascinating field, and I hope to contribute to its growth. -->
                <!-- and specifically in unsupervised learning and semi/self-supervised learning.  -->
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:75%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">

            <p>
              During my undergrad, I worked as an independent researcher at IUBAT CSE Robot-Vision Lab and 
              also as an undergraduate Research Assistant (RA) for multiple projects at the 
              <a href="https://iubat.edu/miyan-research-institute/">Miyan Research Institute</a>  
              under the supervision of 
              <a href="https://cse.iubat.edu/rashedul-islam/">Prof. Rashedul Islam</a>, 
              <a href="https://cse.iubat.edu/md-alomgir-hossain/">Prof. Md. Alomgir Hossain</a> and 
              <a href="https://cs.aiub.edu/profile/aminun.nahar">Prof. Aminun Nahar</a> (from 2017 to 2020).
            </p>
            <p>After my graduation, I was fortunate to get an opportunity to work as a Research Assistant 
              under the guidance of <a href="https://juniv.edu/teachers/ezharul.islam">Prof. Dr. Md. Ezharul Islam</a> 
              at <a href="https://juniv.edu/">Jahangirnagar University</a> 
              for a novel research project of Deep Learning based Computer Vision and Robotics in 2021 which led us winning the 
              <strong>
                <a href="https://drive.google.com/file/d/1Lx_0N-qDk464qy2mpFBdnzqiLk6QxPLf/view">Best Paper Award</a>
              </strong> 
              in the International Conference of MIDAS 2021, Springer.
            </p>
            <p>
              Later on, between 2021 to 2024, I worked as a Researcher and Deep Learning Algorithm Developer in  
              <a href="https://www.cisscom.com/">Cisscom</a> (United States),
              <a href="https://kaleidosoftgames.com/">KaleidoSoft</a> (Croatia) and 
              <a href="https://www.vinacts.com/">Vinacts</a> (South Korea).
    
              Currently, I am working as a Research Assistant (RA) 
              with <a href="https://www.linkedin.com/in/humayun-kabir-phd-49696515a/?originalSubdomain=kr">Prof. Dr. Humayun 
                Kabir</a>
              from the Department of Integrated System Engineering at <a href="https://eng.inha.ac.kr/eng/index.do">Inha 
                University</a>, South Korea. 
              My research field relates with
              developing state of the art efficient algorithms for Robot Vision capabilities 
              and medical image processing; occlusion aware object tracking and frame interpolation mechanism. 
              
              
            </p>
          </td>
        </tr>
      </tbody></table>



        <table style="width:75%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
                <heading><strong>Publications</strong></heading><br><br>
              
            </td>
          </tr>
        </tbody></table>

        <!-- Papers list -->


        <table style="width:75%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="nlt_stop()" onmouseover="nlt_start()">
           <td style="padding:20px;width:25%;vertical-align:middle">
             <img src="sbhk-net-graphics.PNG" width="200" height="140">
           </td>
           <td width="75%" valign="middle">
             <a href= "https://doi.org/10.36227/techrxiv.171710259.90201373/v1">
               <papertitle>
                YOLO/SBHK-Net: Occlusion Aware Robot Vision Neural Network with Compound Model Scaling and Re-parameterized Convolution</papertitle>
             </a>
             <br>
               <strong>Sheekar Banerjee</strong>, Humayun Kabir<br>
             <!-- <em>Submitted to IEEE Transactions on Neural Networks and Learning Systems in May 2024</em><br> -->
             [<a href="https://doi.org/10.36227/techrxiv.171710259.90201373/v1">techRxiv</a>]/
             [<a href="https://github.com/ac005sheekar/SBHK-Net">code</a>]
             <p> In this research, we initiated a unique and cutting-edge backbone neural network for the conventional YOLO algorithm which we named as SBHK-Net. The network boosted up the performance of the existing YOLO algorithm drastically which manifests a strong potential of improving tracking and recognition accuracies of other conventional algorithms in the robot vision industry as well. It has the greatest accuracy 59.2% AP among all known real-time object detectors with 30 FPS or above on GPU RTX3060, and it outperforms all other known object detectors in the range of 5 FPS to 160 FPS. We used YOLOv7 as our reference point for the core research. The transformer-based detector SWINL Cascade-Mask R-CNN (9.2 FPS A100, 53.9% AP) and the convolutional detector ConvNeXt-XL Cascade-Mask R-CNN (8.6 FPS A100, 55.2% AP) are both outperformed by the SBHK-Net core object detector (56 FPS RTX3060, 56.4% AP) in terms of speed and accuracy, respectively.</p>
             
           </td>
         </tr> </tbody></table>

<br><br>


        <table style="width:75%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="nlt_stop()" onmouseover="nlt_start()">
           <td style="padding:20px;width:25%;vertical-align:middle">
             <img src="breast 2020 CNN.png" width="200" height="140">
           </td>
           <td width="75%" valign="middle">
             <a href= "https://www.biorxiv.org/content/10.1101/2024.05.04.592536v1.abstract">
               <papertitle>
                An Introductory Implementation of Breast Cancer Detection from Mammograms and Pixel Intensity with Efficient-Net Other Neural Nets</papertitle>
             </a>
             <br>
               <strong>Sheekar Banerjee</strong>, Humayun Kabir<br>
             <em>Published in Cold Spring Harbor Laboratory, 2024</em><br>
             [<a href="https://www.biorxiv.org/content/10.1101/2024.05.04.592536v1.full.pdf">bioRxiv</a>]/
             [<a href="https://github.com/ac005sheekar/Breast-Cancer-Detection-with-Pixel-Intensity/">code</a>]
             <p> We focused upon the deep learning approach to classify the normal and abnormal breast according to the medical imaging from the MIAS dataset of Mammograms and Pixel Intensity. The Convolution Neural Network (CNN) alongside ResNet, AmoebaNet and EfficientNet have been used for the detection with 330 mammograms in which 194 images are normal and 136 are having the identification of abnormal breasts. The accuracy of the entire experimental results was carrying the torch of potential legacy of deep learning in the medical imaging arena. The research is ongoing for the further development and optimization of CNN, AmoebaNet-C and EfficientNet architecture for the Pixel Intensity with higher accuracy, proper segmentation and masking.</p>
             
           </td>
         </tr> </tbody></table>

<br><br>

        
          


          <table style="width:75%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
           <tr onmouseout="nlt_stop()" onmouseover="nlt_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="breast.PNG" width="200" height="140">
            </td>
            <td width="75%" valign="middle">
              <a href= "https://link.springer.com/chapter/10.1007/978-3-031-53717-2_30">
                <papertitle>CEIMVEN: An Approach of Cutting Edge Implementation of Modified Versions of EfficientNet (V1-V2) Architecture for Breast Cancer Detection and Classification from Ultrasound Images</papertitle>
              </a>
              <br>
                <strong>Sheekar Banerjee</strong>, Md. Kamrul Hasan Monir<br>
              <em>Presented at the 2nd International Conference on Computing, IoT and Data Analytics (<strong> <a href="https://iccida.net/">ICCIDA</a></strong>)</em>, 2023.<br>
              <em>Published in the Studies in Computational Intelligence, Springer-Nature, Switzerland, 2024</em><br>
              [<a href="https://arxiv.org/abs/2308.13356">arXiv</a>]
              [<a href="https://docs.google.com/presentation/d/1TERuOyXkmCEQR01EPGFrGaff_N2szD9b/edit?usp=sharing&ouid=113883945430849884931&rtpof=true&sd=true">slides</a>] /
              [<a href="https://github.com/ac005sheekar/CEIMVEN">code</a>]
              <p> In this research, we focused mostly on our rigorous novel implementations and iterative result analysis of different cutting-edge modified versions of EfficientNet architectures namely EfficientNet-V1 (b0-b7) and EfficientNet-V2 (b0-b3) with ultrasound image, named as CEIMVEN. We utilized transfer learning approach here for using the pre-trained models of EfficientNet versions. The approximate testing accuracies we got from the modified versions of EfficientNet-V1 (b0- 99.15%, b1- 98.58%, b2- 98.43%, b3- 98.01%, b4- 98.86%, b5- 97.72%, b6- 97.72%, b7- 98.72%) and EfficientNet-V2 (b0- 99.29%, b1- 99.01%, b2- 98.72%, b3- 99.43%) are showing very bright future and strong potentials of deep learning approach for the successful detection and classification of breast cancers from the ultrasound images at a very early stage.</p>
              
            </td>
          </tr> </tbody></table>

<br><br>

          <table style="width:75%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
           <tr onmouseout="nlt_stop()" onmouseover="nlt_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="combo_nano.png" width="200" height="140">
            </td>
            <td width="75%" valign="middle">
              <a href="https://link.springer.com/chapter/10.1007/978-981-19-2347-0_55">
                <papertitle>Nano Rover: A Multi-Sensory Full-Functional
                    Surveillance Robot with Modified Inception-Net</papertitle>
              </a>
              <br>
                <strong>Sheekar Banerjee</strong>, Aminun Nahar Jhumur, Md. Ezharul Islam<br>
              <em>Presented at the Machine Intelligence and Data Science Applications, 
                Proceedings of <strong><a href="https://link.springer.com/book/10.1007/978-981-19-2347-0">MIDAS 2021</a></strong></em>
              
              <em><strong>(<a href="https://drive.google.com/file/d/1Lx_0N-qDk464qy2mpFBdnzqiLk6QxPLf/view">Best Paper Award winner</a>)</strong></font>.</em><br>
              <em>Published in the Lecture Notes on Data Engineering and Communications Technologies, Springer-Nature, Singapore, 2022</em><br>
                
                [<a href="https://docs.google.com/presentation/d/1ASyFQ6cF2blp3uGTeCa4RvwnLy2elrsD/edit?usp=sharing&ouid=113883945430849884931&rtpof=true&sd=true">slides</a>]
                
              <p> Nano Rover is a significant approach of cost-efficient surveillance and reconnaissance robot which is fully functional and cost-efficient at the same time. It features the service of active reconnaissance mode with LIDAR sensor, location tracking with GPS Neo 6M module, visual information collection, person detection, weapons detection and identification, gender and age prediction of the hostile and other artificial threat detection, etc. Remote navigation plays as the core controlling system of the robot which is also modifiable through replacement with Internet and satellite navigation system. We modified the conventional Inception-Net architecture with a better hyper-parameter tuning for the successful execution of image processing tasks with a better level of accuracy so far.</p>
            </td>
          </tr> </tbody></table>

<br><br>


          <table style="width:75%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
           <tr onmouseout="nlt_stop()" onmouseover="nlt_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="submarine.PNG" width="200" height="140">
            </td>
            <td width="75%" valign="middle">
              <a href="https://tis.wu.ac.th/index.php/tis/article/view/4205">
                <papertitle>A Novel Approach of Marine Ecosystem Monitoring System with Multi-Sensory Submarine on Robotic Platform for Visualizing the Climate Change Effect over Oceanic Environment</papertitle>
              </a>
              <br>
                <strong>Sheekar Banerjee</strong>, Aminun Nahar Jhumur<br>
              <em>Trends in Sciences (<strong><a href="https://tis.wu.ac.th/index.php/tis/">TiS</a> </strong>)</em>, 2022 <br>
              
              <p> This robotics research project proposes a solution which appears to be a full-fledged Bluetooth controlled Submarine prototype with a sensory chipboard attached inside its endo-skeleton which contains multiple sensors like DHT11 temperature-humidity, dust, CO2 and YL69 pH sensors. The sensory data provides the information of underwater whether the naval environment is habitable for the marine biological species or not, under the terrible effect of global climate change. The submarine prototype is fully functional in the surface and underwater scenario which contains a very unique mechanical design and circuitry with an exceptional sensor data streaming capability which can be used by marine biological researchers and oceanographers professionally as a full-fledged marine ecosystem monitoring device.</p>
            </td>
          </tr> </tbody></table>

<br><br>
            <table style="width:75%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr onmouseout="nlt_stop()" onmouseover="nlt_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="chatbot-graphic.PNG" width="200" height="140">
              </td>
              <td width="75%" valign="middle">
                <a href= "https://www.researchgate.net/profile/Sheekar-Banerjee/publication/380166199_AN_INTEGRAL_EFFORT_OF_OPTIMIZING_THE_MAJOR_INFORMATION_RETRIEVAL_CHATBOT_ALGORITHMS_WITHIN_A_UNIVERSITY_AUTOMATION_PLATFORM/links/662ec0267091b94e93e35fd8/AN-INTEGRAL-EFFORT-OF-OPTIMIZING-THE-MAJOR-INFORMATION-RETRIEVAL-CHATBOT-ALGORITHMS-WITHIN-A-UNIVERSITY-AUTOMATION-PLATFORM.pdf">
                  <papertitle>
                    An Integral Effort of Optimizing the Major Information Retrieval Chatbot Algorithms within a University Automation Platform</papertitle>
                </a>
                <br>
                  <strong>Sheekar Banerjee</strong>, Md. Sakibul Islam<br>
                <em>Undergrad Thesis on Natural Language Processing and Human Computer Interaction, 2020</em><br>
                [<a href="https://www.researchgate.net/profile/Sheekar-Banerjee/publication/380166199_AN_INTEGRAL_EFFORT_OF_OPTIMIZING_THE_MAJOR_INFORMATION_RETRIEVAL_CHATBOT_ALGORITHMS_WITHIN_A_UNIVERSITY_AUTOMATION_PLATFORM/links/662ec0267091b94e93e35fd8/AN-INTEGRAL-EFFORT-OF-OPTIMIZING-THE-MAJOR-INFORMATION-RETRIEVAL-CHATBOT-ALGORITHMS-WITHIN-A-UNIVERSITY-AUTOMATION-PLATFORM.pdf">ResearchGate</a>]/
                [<a href="https://github.com/ac005sheekar/Chatbot-Automation-with-Python-Keras-and-Tensorflow">code</a>]
                <p> In this research, we tried to represent an optimized implementation of different chatbot algorithmic approaches within a single University Automation query platform where the previous chatbots were developed concerning the utility areas of agriculture, economics, medical science with disease prediction, admission system and tourism. They were manifested with Deep Neural Networks like CNN, LSTM and NLP with different environments and approaches. Our primary concern was to combine and optimize all of the approaches in a single platform of a University Automation System as much as possible having Keras as backend. The accuracy rate seems to be very promising through the amalgamation of CNN layers and LSTM with NLP features.</p>
                
              </td>
            </tr> </tbody></table>



<br><br>
            <table style="width:75%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr onmouseout="nlt_stop()" onmouseover="nlt_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="sim.jpg" width="200" height="140">
              </td>
              <td width="75%" valign="middle">
                <a href="https://www.researchgate.net/publication/379537385_The_Smart_Injector_A_Robotic_Approach_of_Automated_Multiple_Injection_System_for_Critical_Patients_using_Real_Time_Clock_Feature">
                  <papertitle>The Smart Injector: A Robotic Approach of
                    Automated Multiple Injection System for
                    Critical Patients using Real Time Clock Feature</papertitle>
                </a>
                <br>
                  <strong>Sheekar Banerjee</strong>, Md. Alomgir Hossain<br>
                  <em>Voluntary Research Work at IUBAT CSE Robotics Club</em>, 2019 <br>
                  [<a href="https://www.researchgate.net/publication/379537385_The_Smart_Injector_A_Robotic_Approach_of_Automated_Multiple_Injection_System_for_Critical_Patients_using_Real_Time_Clock_Feature#fullTextFileContent">ResearchGate</a>]
                  [<a href="https://github.com/ac005sheekar/Smart-Injector">code</a>]
                
                <p> In the rural world of medical services, we
                  generally notice a lot of havoc which generally happens
                  in the hospitals, clinics and related other medical
                  centers. The conditions of Intensive Care Units (ICU) of
                  the rural areas are quite intolerable because of the lack
                  of qualified nurses. Doctors generally prescribe multiple
                  injections for a single patient for each day. Nurses are
                  responsible for the injection process but unfortunately
                  they fail to perform the injection process very often in
                  proper prescribed time in proper amount. This
                  malpractice of treatment quite often results in the
                  terrible sufferings of the ICU patients and sometimes a
                  few patients even die. This study aims to minimize the
                  hazard at the highest accuracy level possible. The
                  research relates to the functionality of Real Time Clock
                  (RTC) which provides the activation of automated time
                  system and triggers the microcontroller's machinery to
                  act according to the time. According to the doctor's
                  prescribed time, injection's medicine will be flowing
                  inside the pipelines and will be injected to the body of
                  the patient through a cannula. In this study, Arduino
                  microcontroller, RTC DS3231 time module, HX 711
                  weight sensors, relay modules, hydraulic pump motors,
                  wifi shield, resistors, MOSFET and breadboard have
                  been used. Following the prescribed time of doctor, the
                  RTC module programs the time for the activation of the
                  microcontroller. The microcontroller activates the
                  hydraulic pump motors following the programmed
                  times. The pump motors then create a vacuum
                  environment inside the pipeline and pass the medicine
                  fluid for injection inside the patient's body through the
                  multiple channel-single cannula. This is consisted of
                  three units: electrical circuitry unit, mechanical unit
                  and timer program with reprogram process unit. The
                  research were carried out through the tests of each and
                  every units to verify that they were working precisely
                  and were manifesting the expected outputs.</p>
              </td>
              
            </tr> </tbody></table>

            

<br><br><br><br>
          
          


          


      
      </td>
    </tr>
  </table>
</body>

</html>
