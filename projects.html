<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Projects - Sheekar Banerjee</title>
  
  <meta name="author" content="Sheekar Banerjee">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css">
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.7.1/dist/jquery.slim.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
  <link rel="icon" type="image/png" href="sheekar2.jpg">
</head>

<body>
  <nav class="navbar navbar-expand-sm bg-dark navbar-dark">
    <!-- Brand/logo -->
    <a class="navbar-brand" href="index.html">Home</a>
    
    <!-- Links -->
    <ul class="navbar-nav">
      <li class="nav-item">
        <a class="nav-link" href="research.html">Research</a>
      </li>
      <li class="nav-item">
        <a class="nav-link" href="projects.html">Projects</a>
      </li>
      <li class="nav-item">
        <a class="nav-link" href="teach.html">Teaching</a>
      </li>
      <li class="nav-item">
        <a class="nav-link" href="leader.html">Competitions</a>
      </li>
      <li class="nav-item">
        <a class="nav-link" href="workshops.html">Workshops</a>
      </li>
      <li class="nav-item">
        <a class="nav-link" href="culture.html">Culture</a>
      </li>
    </ul>
  </nav><br>
  
      <table style="width:75%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading><strong>Projects</strong></heading>
          </td>
        </tr>
      </tbody></table>

      <!-- Papers list -->
      <table style="width:75%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr onmouseout="nlt_stop()" onmouseover="nlt_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="pose.JPG" width="160" height="100">
        </td>
        <td width="75%" valign="middle">
          <!-- <a href="video link to be added..."> -->
            <strong><papertitle>Multiple Person Pose Estimation for Robot Vision and Streaming with Socket Programming to 
              TCP Server</papertitle> [2023]</strong>
          </a>
          <br>
          Supervised by: Geunseong Kim, Development Head at Vinacts, South Korea <br>
            
          <p> This is a very small part of a gigantic confidential South Korean project where 
            multiple persons' pose estimation and keypoint tracking have been done. The data values of the landmarks
            have been streamed to the TCP Server through Socket Programming with Python. YOLO version 5 has been used for 
            pose estimation and tracking. Further improvement has been going on which cannot be shared due to 
            strict project confidentiality.
          </p>
          [<a href="https://youtu.be/PdYfGs-6CJM">Single Person Demo</a>] / 
          [<a href="https://youtu.be/sA4SLYT__oc">Multiple Person Demo</a>] / 
          [<a href="https://github.com/ac005sheekar/Multipose-yolov5-tcp">code</a>]
        </td>
      </tr> </tbody></table>

<br><br>


      <table style="width:75%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr onmouseout="nlt_stop()" onmouseover="nlt_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="recog2.JPG" width="160" height="100">
        </td>
        <td width="75%" valign="middle">
          <!-- <a href="video link to be added..."> -->
            <strong><papertitle>Cutting Edge Object Recognition for Robot Vision and Data feeding to the Unreal Engine with Socket Programming</papertitle> [2023]</strong>
              
          </a>
          <br>
          Supervised by: Geunseong Kim, Development Head at Vinacts, South Korea <br>
            
          <p> This is a code fragment of another large confidential South Korean project where 
            multiple object recognition has been done. The object data values 
            have been streamed to the TCP Server alinged with Unreal Engine 5 environment through Socket Programming 
            with Python. Here, YOLO version 7 has been used for 
            continuous object recognition and tracking. Further improvement has been going on which cannot be shared due to 
            strict project confidentiality.
          </p>
          [<a href="https://youtu.be/jUR3j4k-UaM">Demo 1</a>] / 
          [<a href="https://youtu.be/Egah9hEqD1I">Demo 2</a>] / 
          [<a href="https://github.com/ac005sheekar/YOLO-V7-TCP-Server-Socket-Programming">code</a>]
        </td>
      </tr> </tbody></table>



<br><br>
      <table style="width:75%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr onmouseout="nlt_stop()" onmouseover="nlt_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="facedis.PNG" width="160" height="100">
        </td>
        <td width="75%" valign="middle">
          <!-- <a href="video link to be added..."> -->
            <strong><papertitle>Face Distance Measurement for AI Digital Human's Greetings Feature </papertitle> [2023]</strong>
          </a>
          <br>
          Supervised by: Geunseong Kim, Development Head at Vinacts, South Korea <br>
            
          <p> This is a code fragment of a magnanimous confidential South Korean project where 
            greetings feature has been done with face distance measurement with OpenCV. The distance data values 
            have been streamed to the Flask and TCP Server alinged with Unreal Engine 5 environment through Socket Programming 
            with Python. Further improvement has been going on which cannot be shared due to 
            strict project confidentiality.
          </p>
          [<a href="https://youtu.be/68SmZxU9GW4">Demo 1</a>] / 
          [<a href="https://youtu.be/5BAI8DyfecM">Demo 2</a>] / 
          [<a href="https://youtu.be/gdhrFTPIeLo">Demo 3</a>] / 
          [<a href="https://github.com/ac005sheekar/Face-Distance-OpenCV-TCP-Server-Socket-Programming">code</a>]
        </td>
      </tr> </tbody></table>


<br><br>

        


        <table style="width:75%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="nlt_stop()" onmouseover="nlt_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="alz.PNG" width="160" height="100">
          </td>
          <td width="75%" valign="middle">
            <!-- <a href="https://www.youtube.com/watch?v=ocCYdEAt0do"> -->
              <strong><papertitle>Alzheimer's Disease Stage Detection with Deep Learning in Android App</papertitle> [2022]</strong>
            </a>
            <br>
              Supervised by: <a href="https://www.linkedin.com/in/petar-j-346862208/">Petar Janicic</a>, 
              CEO of KaleidoSoft (Zagreb, Croatia) <br>
              
            <p> This is an Android mobile application prototype where user can upload a Tomographic Brain X-ray image 
              into the system and get an AI based prediction out of it. The prediction mainly relates to the 
              stage detection of Alzheimer's disease such as: Demented, Mild Demented, Very Mild Demented and 
              Non Demented. Here, I implemented the deep learning model with Tensorflow framework, converted the artifact 
              into Tf-Lite and served it to the Android platform.
            </p>
            [<a href="https://www.youtube.com/watch?v=ocCYdEAt0do">Demo</a>] /
            [<a href="https://github.com/ac005sheekar/Alzheimer-s-Disease-Stage-Detection-with-Deep-Learning-in-Android-App">code</a>]
          </td>
        </tr> </tbody></table>

<br><br>

        <table style="width:75%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="nlt_stop()" onmouseover="nlt_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="testjet.PNG" width="160" height="100">
          </td>
          <td width="75%" valign="middle">
            <!-- <a href="https://www.facebook.com/watch/?v=2652088795047266"> -->
              <strong><papertitle>TestJet AI</papertitle> [2021-2022]</strong>
            </a>
            <br>
              Supervised by: <a href="https://theorg.com/org/bangla-institute/org-chart/yameen-ahmad">Yameen Ahmad</a> 
              at Titan Technologies (Sister concern of Cisscom LLC, California, USA) <br>
            <p> TestJet-AI is an AI powered Self-Healing Test Automation System for Mazda-USA within Web and Mobile Platform where I implemented AI- Vision based Image Comparison and OCR Testing capabilities for pixel-by-pixel image verification with Django Rest Framework and core MEVN stack.
              I also designed the Solution Architecture for self-healing web testing with healenium-python and Django.
              The beta version of the project has been launch for the users in the market on February 2022 when I was working at Cisscom LLC. Since then it has been gaining rave reviews from the users and software testers.

            </p>
            <!-- [<a href="https://www.testjet.ai/">Link</a>] -->
            [<a href="https://www.youtube.com/watch?v=Ms1tVs9lPvU">Demo</a>]
          </td>
        </tr> </tbody></table>





<br><br>
        <table style="width:75%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="nlt_stop()" onmouseover="nlt_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="vent.jpg" width="160" height="100">
          </td>
          <td width="75%" valign="middle">
            <!-- <a href="https://www.facebook.com/watch/?v=2652088795047266"> -->
              <strong><papertitle>SENSO CODER Ventilator Simulation with Robotic Arm Approach for COVID-19 Patients</papertitle> [2020]</strong>
            </a>
            <br>
              Supervised by: <a href="https://cse.iubat.edu/rashedul-islam/">
                Prof. Rashedul Islam</a> <br>
            <p> This is a design based simulation project which represents a research proposal of a cost-efficient
              sensor based ventilation system with Robotic Arm Approach, for COVID-19 patients in the
              rural area. On the verge of a massive first wave of COVID-19 outbreak at March'20, studying as a final
              year undergraduate student, I led this simulation project collaborating with three mechanical
              engineering students at my university.
              Demonstrated the features of Leakage Free Oxygen Source, Ventilation Bag attached with
              Robotic Arm with Transverse Angle, Programmed Stepper Motor for real time Frequency Con-
              trol and Precise Air Diffusion Rate and Respiratory Sensor for reading the Breathing Rate of
              the patient.
              Got acclamation from the Professors at my University due to the realistic feasibility of the
              design.
            </p>
            [<a href="https://www.facebook.com/watch/?v=2652088795047266">Demo</a>]
          </td>
        </tr> </tbody></table>

<br><br>

        







      
        


        
      </td>
    </tr>
  </table>
</body>

</html>
